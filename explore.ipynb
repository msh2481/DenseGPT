{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import Annotated, Any, Mapping, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch as t\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from beartype import beartype as typed\n",
    "from beartype.door import die_if_unbearable as assert_type\n",
    "from beartype.typing import Callable, Iterable\n",
    "from beartype.vale import Is\n",
    "from datasets import load_dataset\n",
    "from dvclive.huggingface import DVCLiveCallback\n",
    "from jaxtyping import Bool, Float, Int\n",
    "from torch import Tensor as TT\n",
    "from torch.utils.data import IterableDataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "from gpt import DenseGPTConfig, DenseGPTForCausalLM\n",
    "from utils import explore_batch, fetch_or_ask\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = DenseGPTConfig(\n",
    "    vocab_size=50257,\n",
    "    hidden_size=256,\n",
    "    num_layers=8,\n",
    "    attention_types=[[[\"global\"], 8]],\n",
    "    num_heads=16,\n",
    "    use_dense=True,\n",
    ")\n",
    "# model = DenseGPTForCausalLM(config)\n",
    "# model = DenseGPTForCausalLM.from_pretrained(\"roneneldan/TinyStories-8M\", config=config)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"roneneldan/TinyStories-8M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n",
      "Using pad_token, but it is not set yet.\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"roneneldan/TinyStories\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"roneneldan/TinyStories-8M\")\n",
    "tokenizer.pad_token = tokenizer.pad_token or tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844c0ea6d3c8416e96b7f0512025b1d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@typed\n",
    "def tokenize_function(example: Mapping[str, str | int]) -> Mapping[str, list[int]]:\n",
    "    result = tokenizer(\n",
    "        example[\"text\"],\n",
    "        max_length=256,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"]\n",
    "    return result\n",
    "\n",
    "\n",
    "subset_size = 1000\n",
    "subset = dataset[\"train\"].shuffle().select(range(subset_size))\n",
    "tokenized = subset.map(tokenize_function, batched=True).remove_columns([\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explore_batch(model, tokenizer, tokenized, n_samples=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "049ebc0c55a34ed4837409d688a92383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from utils import get_loss\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "losses = []\n",
    "for row in tqdm(dataset[\"train\"].shuffle().select(range(100))):\n",
    "    losses.append(get_loss(model, tokenizer, row[\"text\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 1.3923834615945816\n"
     ]
    }
   ],
   "source": [
    "print(len(losses), sum(losses) / len(losses))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
